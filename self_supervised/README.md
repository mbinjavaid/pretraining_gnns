# pretraining_gnns

## Structure of repository

- [ ] **pretrain_contextpred:** run this to carry out self-supervised pre-training on the QM9 or ZINC dataset with the Context Prediction technique. Upon running this, a GNN model is trained and saved in the `models/embeddings_pretrain_contextpred/` or `models/multihot_pretrain_contextpred/` folder (depending on whether the `--use_embeddings` parameter was set to 1 or 0).
- [ ] **pretrain_masking:** run this to carry out self-supervised pre-training on the QM9 or ZINC dataset with the Attribute Masking technique. The sub-variant of Attribute Masking can be specified through the `--which_mask` parameter from amongst **atom_type** (AttrMask (Atom)), **func_group** (AttrMask (FG)), and **all** (AttrMask (Atom, FG)). Upon running this, a GNN model is trained and saved in the `models/embeddings_pretrain_masking/` or `models/multihot_pretrain_masking/` folder.
- [ ] **pretrain_supervised:** run this to carry out multi-task supervised pre-training on 12 properties of the QM9 dataset.  Upon running this, a GNN model is trained and saved in the `models/embeddings_pretrain_supervised/` or `models/multihot_pretrain_supervised/` folder. Can also utilize previously pre-trained models generated by pretrain_contextpred or pretrain_masking by setting the `--strategy` parameter to `contextpred` or `masking` respectively. In those cases, will further pre-train using as initialization each of the models saved by that respective technique in the models folder, one by one (only those models which used the same specified `--use_embeddings` setting).
- [ ] **finetune:** run this to fine-tune and test on the downstream datasets, optionally using the trained model(s) saved by any of the pretrain scripts as initialization. Will sequentially fine-tune and test on all the specified downstream datasets for each pre-trained model present which satisfies the criteria of the `--strategy` parameter: contextpred (ContextPred only), contextpred_sup (ContextPred + Supervised), masking (AttrMask), masking_sup (AttrMask + Supervised), sup_only (Supervised), or no_pretrain (no pre-training). The test results along with the model parameters used will be saved in `results/self_supervised/`. The choice of `--strategy` and `--use_embeddings` determines the filename of the CSV. Previously saved results are not overwritten.

## Instructions

At least one of the pretraining scripts must be run before finetune.py, unless using `--strategy no_pretrain` when running finetune.py.

The training and GNN parameters (and the downstream datasets to use during fine-tuning) can be specified, please check details with `python filename.py --help` for the file you wish to run. However, when running finetune.py, except when using `--strategy no_pretrain`, the following input GNN model parameters will be ignored, as the parameters of the saved pre-trained GNN model (generated by one of the pretrain scripts) currently being fine-tuned will automatically be adopted; these parameters include the choice between multi-hot encoding/feature embeddings (`--use_embeddings`), hidden dimension (`--hidden_dim`), number of GNN layers (`--num_layers`) the presence or absence of Batch Normalization (`--batch_norm`), and, if fine-tuning a model output by pretrain_supervised, the choice of sum/mean graph pooling (`--global_pooling`). The situation is similar when running pretrain_supervised with `--strategy masking` or `--strategy contextpred`: then, only the `--global_pooling` parameter can be manually specified.

As referenced in the thesis, the only difference between the GCNs here and in single_multi_task_GCNs is that here, the ReLU activation after the final GNN layer is excluded (in order for ContextPred to work). Additionally, we note that the GINs (GIN and GIN Concat.) have dropout implemented, as indicated in the thesis, but GCN does not; using a GCN model will cause the `--dropout_ratio` setting to be ignored.

**Reproducing the thesis results**: when pretraining, or finetuning with `--strategy no_pretrain`, for GIN and GIN Concat., use `--gnn_type gin` or `--gnn_type ginconcat` accordingly; for the GCNs, change the following settings as indicated: `--gnn_type gcn --hidden_dim 32 --num_layers 4`. For GCN w. BN, set `--batch_norm 1` and for GCN w/o BN, set `--batch_norm 0`. When running finetune.py with any other `--strategy` option, no model or training parameters need to be manually specified. However, note that the `--use_embeddings` parameter (i.e. choice between multi-hot encoding or feature embeddings) can always be manually specified; when finetuning or doing supervised pre-training as a second pretraining step, only saved models which used the same specified `--use_embeddings` setting will be loaded.

To reproduce all our results for a given `--use_embeddings` setting: run pretrain_masking and pretrain_contextpred for each GNN model once, followed by pretrain_supervised with `--strategy` set to `masking` and `contextpred` once each. Also run pretrain_supervised with `--strategy none` to perform only supervised pre-training, once for each GNN model. Then, please use the appropriate `--strategy` setting when running finetune.py to choose which type of pretrained models to finetune; run once for each `--strategy` choice.

## Acknowledgements

This code is largely adapted from the work by Hu et al. titled "Strategies for Pre-training Graph Neural Networks".

Paper: https://arxiv.org/abs/1905.12265

Code: https://github.com/snap-stanford/pretrain-gnns/

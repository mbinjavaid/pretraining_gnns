# pretraining_gnns

## Structure of repository

- [ ] **functional_group_split.py:** Generates 10 functional group splits of each of the downstream datasets, following the specifications in Section 5.3.3 of the thesis. For each dataset, generates a "test_indices" file containing the indices of the molecules to be placed in the test set of each of the 10 splits, as well as a "test_fgs" file which tells the exact index of each functional group partitioned off to create each test set. These files are saved in the `functional_groups` folder.
- [ ] **qm9_fgs_split.py:** Using the files generated by functional_group_split.py, generates indices of QM9 molecules for creating the 10k and 10k Excluded pre-training subsets for each downstream dataset (as specified in Section 5.3.3 of the thesis). These indices are saved in the `functional_groups` folder. 
- [ ] **pretrain.py:** run this to carry out multi-task supervised pre-training with the QM9 dataset. If `10k` or `10kExcluded` is used as the `pretrain_subset` parameter, a pre-trained model is generated for every downstream dataset (as the set of `10k` or `10kExcluded` QM9 molecules is different for every downstream dataset, due to their different functional group splits). If `full` is used as the parameter (i.e. pre-training on the full QM9 dataset), only one pre-trained model is generated. The `--features` parameter allows choosing between chemical features (`chem`) or functional group encodings (`fge`). Upon running this, the GNN model(s) are trained and saved in the `models/fgs_pretrain/` folder.
- [ ] **finetune.py:** run this to fine-tune and test on the downstream datasets using the trained model(s) saved by the pretrain scripts as initialization. If using `--use_pretrained 1`, will sequentially fine-tune and test on each of the specified downstream datasets for each pre-trained model present in the models/fgs_pretrain folder, adapting the same model parameters and features choice (`chem` or `fge`) as each pretrained model. If setting `--use_pretrained 0`, will finetune and test without using any pretraining. The `--split` parameter allows choosing between functional group splits (`fg`) and random splits (`random`). The test results along with the model parameters used will be saved in  the `results/functional_groups` folder. The features used and `--split` determines the filename of the CSV in which any given model's test results are saved. Previously saved results are not overwritten.

## Instructions

First run **functional_group_split.py**, followed by **qm9_fgs_split.py**, followed by **pretrain.py**, and finally **finetune.py**.

The training and GNN parameters can be specified, please check details with `python filename.py --help` for the file you wish to run. However, when running finetune.py, except when using `--use_pretrained 0`, the specified input GNN model parameters will be ignored, as the parameters of the pre-trained GNN model currently being fine-tuned will automatically be adopted; these parameters include the choice between chemical features/functional group encoding, hidden dimension, number of GNN layers, choice of graph pooling function, and the presence or absence of Batch Normalization. Additionally, we note that the GINs (GIN and GIN Concat.) have dropout implemented, but GCN, as indicated in the thesis, does not; using a GCN model will cause the `--dropout_ratio` setting to be ignored. The GNN architectures here are the same as those in self_supervised.

In **finetune.py**, the three lists in lines `110-113` (`target_properties_downstream`, `abbreviations`, and `dataset_test_indices`) determine which downstream datasets are used for fine-tuning/testing, as well as the order in which this is done. Please make sure to change all three lists accordingly if you wish to modify this; the datasets referred to and their ordering must match between them!

**Reproducing the thesis results**: When pretraining, choose the desired `--pretrain_subset` (`10k`, `10kExcluded`, or `full`) and `--features` (`chem` or `fge`). When pretraining, or finetuning with `--use_pretrained 0`, for GIN and GIN Concat., use `--gnn_type gin` or `--gnn_type ginconcat` accordingly; for the GCNs, use the following settings: `--gnn_type gcn --hidden_dim 32 --num_layers 4`. For GCN w. BN, set `--batch_norm 1` and for GCN w/o BN, set `--batch_norm 0`.  When finetuning, choose the `--split` according to which results to reproduce. When finetuning with `--use_pretrained 1`, no GNN type or GNN parameters need to be manually specified; those of the pre-trained model(s) will automatically be adopted.

To reproduce all our results, run pretrain.py with each of the `--pretrain_subset` options once for each GNN model and `--features` option, then run finetune.py once with `--use_pretrained 1 --split fg`, and once with `--use_pretrained 1 --split random`. Run finetune.py with `--use_pretrained 0` once for each possible combination of `--features` and `--split` to obtain the non-pretrained results in all cases (using FGEs/chemical features and a functional group split/random split).
